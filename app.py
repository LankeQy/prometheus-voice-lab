
import gradio as gr
import torch
import torchaudio
from speechbrain.pretrained import EncoderClassifier
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
import yt_dlp
import os
import uuid
import traceback

# ---- 1. æ¨¡å‹åŠ è½½ ----
print("æ­£åœ¨åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œè¿™å°†éœ€è¦å‡ åˆ†é’Ÿ...")
device = "cuda:0" if torch.cuda.is_available() else "cpu"

try:
    speaker_model = EncoderClassifier.from_huggingface("speechbrain/spkrec-xvect-voxceleb", run_opts={"device": device})
    print("âœ… å£°çº¹æå–æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"ğŸ”´ å£°çº¹æå–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    speaker_model = None

try:
    tts_processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
    tts_model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to(device)
    tts_vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
    print("âœ… TTS è¯•å¬æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"ğŸ”´ TTS è¯•å¬æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    tts_model = None

print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œåº”ç”¨å‡†å¤‡å°±ç»ªï¼")


# ---- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
def process_audio_and_get_name(filepath, source_info="file"):
    if filepath is None: return None, None
    print(f"æ­£åœ¨å¤„ç†æ¥è‡ª '{source_info}' çš„éŸ³é¢‘: {filepath}")
    try:
        signal, fs = torchaudio.load(filepath)
        if fs != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
            signal = resampler(signal)
        if signal.shape[0] > 1:
            signal = torch.mean(signal, dim=0, keepdim=True)
        source_name = os.path.splitext(os.path.basename(filepath))[0]
        if source_info in ["YouTube", "microphone_temp"]:
            try:
                os.remove(filepath)
                print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {filepath}")
            except Exception as e:
                print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        return signal, source_name
    except Exception as e:
        traceback.print_exc()
        raise gr.Error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")

def download_youtube_audio(youtube_url):
    if not youtube_url: return None
    print(f"æ­£åœ¨ä» URL ä¸‹è½½: {youtube_url}")
    temp_filename = f"temp_audio_{uuid.uuid4().hex}"
    ydl_opts = {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav'}], 'outtmpl': temp_filename, 'quiet': True, 'nocheckcertificate': True}
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        output_path = f"{temp_filename}.wav"
        if not os.path.exists(output_path):
             possible_files = [f for f in os.listdir('.') if f.startswith(temp_filename)]
             if not possible_files: raise FileNotFoundError("yt-dlp ä¸‹è½½åæœªæ‰¾åˆ°ä»»ä½•éŸ³é¢‘æ–‡ä»¶ã€‚")
             os.rename(possible_files[0], output_path)
        print(f"URL éŸ³é¢‘å·²ä¸‹è½½åˆ°: {output_path}")
        return output_path
    except Exception as e:
        traceback.print_exc()
        raise gr.Error(f"URL ä¸‹è½½å¤±è´¥: {e}")

def generate_and_test(audio_file, mic_input, youtube_url, text_to_speak):
    if not any([audio_file, mic_input, youtube_url]):
        raise gr.Error("è¯·æä¾›ä¸€ä¸ªéŸ³é¢‘æºï¼šä¸Šä¼ æ–‡ä»¶ã€å½•éŸ³æˆ–è§†é¢‘é“¾æ¥ã€‚")
    if not text_to_speak:
        raise gr.Error("è¯·è¾“å…¥è¦è¯•å¬çš„æ–‡æœ¬ã€‚")
    if speaker_model is None or tts_model is None:
        raise gr.Error("æ ¸å¿ƒæ¨¡å‹æœªèƒ½åŠ è½½ï¼Œåº”ç”¨æ— æ³•å·¥ä½œã€‚")
    waveform, source_name = None, "audio"
    if youtube_url:
        youtube_filepath = download_youtube_audio(youtube_url)
        waveform, source_name = process_audio_and_get_name(youtube_filepath, "YouTube")
    elif audio_file is not None:
        waveform, source_name = process_audio_and_get_name(audio_file.name, "file")
    elif mic_input is not None:
        waveform, source_name = process_audio_and_get_name(mic_input, "microphone_temp")
        source_name = "mic_recording"
    print("æ­£åœ¨ç”Ÿæˆå£°çº¹...")
    with torch.no_grad():
        embedding = speaker_model.encode_batch(waveform.to(device))
        embedding = torch.nn.functional.normalize(embedding, dim=2)
        final_embedding = embedding[0][0]
    pt_filename = f"{source_name}_embedding.pt"
    torch.save(final_embedding, pt_filename)
    print(f"å£°çº¹æ–‡ä»¶å·²ä¿å­˜: {pt_filename}")
    print("æ­£åœ¨è¿›è¡ŒTTSè¯•å¬...")
    inputs = tts_processor(text=text_to_speak, return_tensors="pt").to(device)
    speaker_embeddings_for_tts = final_embedding.unsqueeze(0).to(device)
    speech = tts_model.generate_speech(inputs["input_ids"], speaker_embeddings_for_tts, vocoder=tts_vocoder)
    test_audio_filename = f"test_{source_name}.wav"
    torchaudio.save(test_audio_filename, speech.cpu().unsqueeze(0), 16000)
    print(f"è¯•å¬éŸ³é¢‘å·²ç”Ÿæˆ: {test_audio_filename}")
    return pt_filename, test_audio_filename

# ---- 3. Gradio ç•Œé¢å®šä¹‰
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš€ æ™®ç½—ç±³ä¿®æ–¯æ——èˆ°å£°éŸ³å®éªŒå®¤")
    gr.Markdown("åœ¨è¿™é‡Œç”Ÿäº§ã€å¹¶å³æ—¶æµ‹è¯•ç”¨äºæ‚¨ AI å¤§è„‘çš„ä»»ä½•å£°éŸ³ã€‚")
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### 1. æä¾›å£°éŸ³æº (ä¸‰é€‰ä¸€)")
            with gr.Tabs():
                with gr.TabItem("ğŸ“ ä¸Šä¼ æ–‡ä»¶"):
                    audio_file_input = gr.File(label="æ”¯æŒ WAV, MP3, M4A ç­‰æ ¼å¼")
                with gr.TabItem("ğŸ”— è§†é¢‘å¹³å°é“¾æ¥"):
                    youtube_input = gr.Textbox(label="ç²˜è´´ YouTube, Bilibili, æŠ–éŸ³ç­‰ URL", placeholder="https://www.bilibili.com/video/BV...")
                with gr.TabItem("ğŸ¤ éº¦å…‹é£å½•åˆ¶"):
                    mic_input = gr.Audio(sources=["microphone"], type="filepath", label="ç‚¹å‡»å½•åˆ¶ä½ çš„å£°éŸ³")
            gr.Markdown("### 2. è¾“å…¥è¯•å¬æ–‡æœ¬")
            text_input = gr.Textbox(label="è¾“å…¥è¦è¯•å¬çš„ä¸­æ–‡", value="ä½ å¥½ï¼Œæˆ‘æ˜¯æ™®ç½—ç±³ä¿®æ–¯ã€‚è¿™æ˜¯æˆ‘çš„æ–°å£°éŸ³ã€‚")
            generate_btn = gr.Button("ç”Ÿæˆå¹¶è¯•å¬", variant="primary")
        with gr.Column(scale=1):
            gr.Markdown("### 3. è·å–ç»“æœ")
            pt_output = gr.File(label="ä¸‹è½½å£°çº¹ (.pt æ–‡ä»¶)")
            audio_output = gr.Audio(label="è¯•å¬å…‹éš†æ•ˆæœ", type="filepath")
    generate_btn.click(
        fn=generate_and_test,
        inputs=[audio_file_input, mic_input, youtube_input, text_input],
        outputs=[pt_output, audio_output],
        api_name="generate"
    )

# ---- 4. å¯åŠ¨åº”ç”¨ ----
demo.launch()