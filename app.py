# app.py (Super-Flagship Version - Unabridged)
import gradio as gr
import torch
import torchaudio
from speechbrain.inference.speaker import SpeakerRecognition
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
from pyannote.audio import Pipeline
import yt_dlp
import os
import uuid
import traceback

# ---- 1. æ¨¡å‹åŠ è½½ ----
print("æ­£åœ¨åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œè¿™å°†éœ€è¦å‡ åˆ†é’Ÿ...")
device = "cuda:0" if torch.cuda.is_available() else "cpu"
# ä» Hugging Face Space çš„ Secrets ä¸­å®‰å…¨åœ°è·å–ä»¤ç‰Œ
HF_TOKEN = os.environ.get("HF_TOKEN")

# å£°çº¹æå–æ¨¡å‹
try:
    speaker_model = SpeakerRecognition.from_huggingface("speechbrain/spkrec-xvect-voxceleb",
                                                        run_opts={"device": device})
    print("âœ… å£°çº¹æå–æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"ğŸ”´ å£°çº¹æå–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    speaker_model = None

# TTS è¯•å¬æ¨¡å‹
try:
    tts_processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
    tts_model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to(device)
    tts_vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
    print("âœ… TTS è¯•å¬æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"ğŸ”´ TTS è¯•å¬æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    tts_model = None

# VAD "æ™ºèƒ½å‰¯é©¾" æ¨¡å‹
if HF_TOKEN:
    try:
        vad_pipeline = Pipeline.from_pretrained("pyannote/voice-activity-detection", use_auth_token=HF_TOKEN)
        vad_pipeline.to(torch.device(device))
        print("âœ… VAD 'æ™ºèƒ½å‰¯-é©¾' æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"ğŸ”´ VAD 'æ™ºèƒ½å‰¯é©¾' æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        vad_pipeline = None
else:
    vad_pipeline = None
    print("ğŸ”´ è­¦å‘Š: æœªåœ¨ Space Secrets ä¸­æ‰¾åˆ° HF_TOKENã€‚è‡ªåŠ¨è£å‰ªåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œåº”ç”¨å‡†å¤‡å°±ç»ªï¼")


# ---- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ----

def find_best_audio_segment(filepath: str, target_duration: float = 25.0):
    """
    ä½¿ç”¨ VAD æ¨¡å‹æ‰¾åˆ°éŸ³é¢‘ä¸­æœ€é•¿çš„è¿ç»­è¯­éŸ³ç‰‡æ®µï¼Œå¹¶è£å‰ªæˆç›®æ ‡æ—¶é•¿ã€‚
    è¿”å›ä¸€ä¸ª torch.Tensor æ³¢å½¢ã€‚
    """
    if vad_pipeline is None:
        raise gr.Error("VAD æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è‡ªåŠ¨è£å‰ªã€‚è¯·æ£€æŸ¥ HF_TOKEN Secret è®¾ç½®åŠæœåŠ¡å™¨æ—¥å¿—ã€‚")

    print("VAD æ­£åœ¨åˆ†æéŸ³é¢‘ä»¥å¯»æ‰¾æœ€ä½³ç‰‡æ®µ...")
    try:
        waveform, sample_rate = torchaudio.load(filepath)

        # ç¡®ä¿é‡‡æ ·ç‡æ˜¯16kHzï¼Œå› ä¸ºVADæ¨¡å‹éœ€è¦å®ƒ
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
            sample_rate = 16000

        # è¿è¡ŒVAD
        output = vad_pipeline({"waveform": waveform, "sample_rate": sample_rate})
        speech_regions = output.get_timeline().support()

        if not speech_regions:
            raise ValueError("åœ¨éŸ³é¢‘ä¸­æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³æ´»åŠ¨ã€‚")

        # åˆå¹¶é—´éš™å°äº0.5ç§’çš„è¯­éŸ³ç‰‡æ®µ
        merged_regions = []
        if speech_regions:
            current_region = speech_regions[0]
            for next_region in speech_regions[1:]:
                if next_region.start - current_region.end < 0.5:
                    current_region = current_region.union(next_region)
                else:
                    merged_regions.append(current_region)
                    current_region = next_region
            merged_regions.append(current_region)

        # æ‰¾åˆ°æœ€é•¿çš„åˆå¹¶åçš„ç‰‡æ®µ
        longest_region = max(merged_regions, key=lambda region: region.duration)
        print(
            f"æ‰¾åˆ°æœ€é•¿è¯­éŸ³ç‰‡æ®µï¼šæ—¶é•¿ {longest_region.duration:.2f} ç§’ï¼Œä» {longest_region.start:.2f}s åˆ° {longest_region.end:.2f}sã€‚")

        # ä»è¿™ä¸ªç‰‡æ®µä¸­è£å‰ªå‡ºç›®æ ‡æ—¶é•¿çš„éƒ¨åˆ†
        start_frame = int(longest_region.start * sample_rate)
        end_frame = int(longest_region.end * sample_rate)

        # å¦‚æœç‰‡æ®µæœ¬èº«æ¯”ç›®æ ‡é•¿ï¼Œå°±ä»ä¸­é—´æˆªå–
        if longest_region.duration > target_duration:
            mid_frame = (start_frame + end_frame) // 2
            half_duration_frames = int(target_duration * sample_rate / 2)
            start_frame = max(0, mid_frame - half_duration_frames)
            end_frame = mid_frame + half_duration_frames

        clipped_waveform = waveform[:, start_frame:end_frame]

        print(f"å·²è‡ªåŠ¨è£å‰ªå‡º {clipped_waveform.shape[1] / sample_rate:.2f} ç§’çš„é»„é‡‘ç‰‡æ®µã€‚")
        return clipped_waveform, sample_rate

    except Exception as e:
        traceback.print_exc()
        raise gr.Error(f"VAD æ™ºèƒ½åˆ†æå¤±è´¥: {e}")


def download_and_process_link(url: str):
    """ä»é“¾æ¥ä¸‹è½½å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    if not url or not (url.startswith("http://") or url.startswith("https://")):
        return None
    print(f"æ­£åœ¨ä» URL ä¸‹è½½: {url}")
    temp_filename = f"temp_audio_{uuid.uuid4().hex}"

    ydl_opts = {
        'format': 'bestaudio/best',
        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav'}],
        'outtmpl': temp_filename,
        'quiet': True,
        'nocheckcertificate': True,
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])

        output_path = f"{temp_filename}.wav"
        if not os.path.exists(output_path):
            possible_files = [f for f in os.listdir('.') if f.startswith(temp_filename)]
            if not possible_files:
                raise FileNotFoundError("yt-dlp ä¸‹è½½åæœªæ‰¾åˆ°ä»»ä½•éŸ³é¢‘æ–‡ä»¶ã€‚")
            os.rename(possible_files[0], output_path)

        print(f"URL éŸ³é¢‘å·²ä¸‹è½½åˆ°: {output_path}")
        return output_path
    except Exception as e:
        traceback.print_exc()
        raise gr.Error(f"URL ä¸‹è½½å¤±è´¥: {e}")


def generate_and_test(audio_file, mic_audio, youtube_url, text_to_speak, auto_clip):
    """ç»ˆæå‡½æ•°ï¼šå¤„ç†æ‰€æœ‰è¾“å…¥æºï¼Œç”Ÿæˆå£°çº¹ï¼Œå¹¶è¿›è¡ŒTTSæµ‹è¯•"""
    if not any([audio_file, mic_audio, youtube_url]):
        raise gr.Error("è¯·æä¾›ä¸€ä¸ªéŸ³é¢‘æºï¼šä¸Šä¼ æ–‡ä»¶ã€å½•éŸ³æˆ–è§†é¢‘é“¾æ¥ã€‚")
    if not text_to_speak:
        raise gr.Error("è¯·è¾“å…¥è¦è¯•å¬çš„æ–‡æœ¬ã€‚")
    if speaker_model is None or tts_model is None:
        raise gr.Error("æ ¸å¿ƒæ¨¡å‹æœªèƒ½åŠ è½½ï¼Œåº”ç”¨æ— æ³•å·¥ä½œã€‚è¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—ã€‚")

    filepath = None
    source_name = "audio"
    is_temp_file = False

    try:
        # 1. è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        if youtube_url:
            filepath = download_and_process_link(youtube_url)
            source_name = os.path.splitext(os.path.basename(filepath))[0]
            is_temp_file = True
        elif audio_file is not None:
            filepath = audio_file.name
            source_name = os.path.splitext(os.path.basename(filepath))[0]
        elif mic_audio is not None:
            filepath = mic_audio
            source_name = "mic_recording"
            is_temp_file = True  # Gradioçš„éº¦å…‹é£å½•éŸ³ä¹Ÿæ˜¯ä¸´æ—¶æ–‡ä»¶

        if not filepath:
            raise gr.Error("æ— æ³•è·å–æœ‰æ•ˆçš„éŸ³é¢‘æºã€‚")

        # 2. æ ¸å¿ƒæ­¥éª¤ï¼šåŠ è½½å¹¶å¤„ç†éŸ³é¢‘æ³¢å½¢
        if auto_clip:
            waveform, sample_rate = find_best_audio_segment(filepath)
        else:
            print("ç”¨æˆ·é€‰æ‹©æ‰‹åŠ¨æ¨¡å¼ï¼Œæ­£åœ¨åŠ è½½å®Œæ•´éŸ³é¢‘...")
            waveform, sample_rate = torchaudio.load(filepath)

        # ç¡®ä¿æ˜¯16kHz, å•å£°é“
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # 3. ç”Ÿæˆå£°çº¹
        print("æ­£åœ¨ç”Ÿæˆå£°çº¹...")
        with torch.no_grad():
            embedding = speaker_model.encode_batch(waveform.to(device))
            embedding = torch.nn.functional.normalize(embedding, dim=2)
            final_embedding = embedding[0][0]

        pt_filename = f"{source_name}_embedding.pt"
        torch.save(final_embedding, pt_filename)
        print(f"å£°çº¹æ–‡ä»¶å·²ä¿å­˜: {pt_filename}")

        # 4. è¿›è¡ŒTTSè¯•å¬
        print("æ­£åœ¨è¿›è¡ŒTTSè¯•å¬...")
        inputs = tts_processor(text=text_to_speak, return_tensors="pt").to(device)
        speaker_embeddings_for_tts = final_embedding.unsqueeze(0).to(device)

        speech = tts_model.generate_speech(
            inputs["input_ids"],
            speaker_embeddings_for_tts,
            vocoder=tts_vocoder
        )

        test_audio_filename = f"test_{source_name}.wav"
        torchaudio.save(test_audio_filename, speech.cpu().unsqueeze(0), 16000)
        print(f"è¯•å¬éŸ³é¢‘å·²ç”Ÿæˆ: {test_audio_filename}")

        return pt_filename, test_audio_filename

    finally:
        # 5. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        if is_temp_file and filepath and os.path.exists(filepath):
            try:
                os.remove(filepath)
                print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {filepath}")
            except Exception as e:
                print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


# ---- 3. Gradio ç•Œé¢å®šä¹‰ (è¶…æ——èˆ°ç‰ˆ) ----
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš€ æ™®ç½—ç±³ä¿®æ–¯è¶…æ——èˆ°å£°éŸ³å®éªŒå®¤")
    gr.Markdown("æ™ºèƒ½æå–ã€ä¸€é”®å…‹éš†ã€å³æ—¶è¯•å¬â€”â€”åœ¨è¿™é‡Œï¼Œæ‰“é€ æ‚¨AIçš„å®Œç¾å£°éŸ³ã€‚")

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### 1. æä¾›å£°éŸ³æº (ä¸‰é€‰ä¸€)")
            with gr.Tabs():
                with gr.TabItem("ğŸ“ ä¸Šä¼ æ–‡ä»¶"):
                    audio_file_input = gr.File(label="æ”¯æŒ WAV, MP3, M4A ç­‰æ ¼å¼")
                with gr.TabItem("ğŸ”— è§†é¢‘å¹³å°é“¾æ¥"):
                    youtube_input = gr.Textbox(label="ç²˜è´´ YouTube, Bilibili, æŠ–éŸ³ç­‰ URL",
                                               placeholder="https://www.bilibili.com/video/BV...")
                with gr.TabItem("ğŸ¤ éº¦å…‹é£å½•åˆ¶"):
                    mic_input = gr.Audio(sources=["microphone"], type="filepath", label="ç‚¹å‡»å½•åˆ¶ä½ çš„å£°éŸ³")

            # æ™ºèƒ½å‰¯é©¾å¼€å…³
            auto_clip_checkbox = gr.Checkbox(label="ğŸ¤– è‡ªåŠ¨æå–25ç§’é»„é‡‘ç‰‡æ®µ (æ¨è)", value=True,
                                             interactive=vad_pipeline is not None)
            if vad_pipeline is None:
                gr.Markdown("<p style='color:red;'>ğŸ”´ è‡ªåŠ¨æå–åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ HF_TOKEN Secret è®¾ç½®ã€‚</p>")

            gr.Markdown("### 2. è¾“å…¥è¯•å¬æ–‡æœ¬")
            text_input = gr.Textbox(label="è¾“å…¥è¦è¯•å¬çš„ä¸­æ–‡", value="ä½ å¥½ï¼Œæˆ‘æ˜¯æ™®ç½—ç±³ä¿®æ–¯ã€‚è¿™æ˜¯æˆ‘çš„æ–°å£°éŸ³ã€‚")

            generate_btn = gr.Button("ç”Ÿæˆå¹¶è¯•å¬", variant="primary")

        with gr.Column(scale=1):
            gr.Markdown("### 3. è·å–ç»“æœ")
            pt_output = gr.File(label="ä¸‹è½½å£°çº¹ (.pt æ–‡ä»¶)")
            audio_output = gr.Audio(label="è¯•å¬å…‹éš†æ•ˆæœ", type="filepath")

    generate_btn.click(
        fn=generate_and_test,
        inputs=[audio_file_input, mic_input, youtube_input, text_input, auto_clip_checkbox],
        outputs=[pt_output, audio_output],
        api_name="generate"
    )

# ---- 4. å¯åŠ¨åº”ç”¨ ----
demo.launch()